# Yolov5 + Deep Sort with PyTorch





<div align="center">
<p>
<img src="MOT16_eval/track_pedestrians.gif" width="400"/> <img src="MOT16_eval/track_all.gif" width="400"/> 
</p>
<br>
<div>
<a href="https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/actions"><img src="https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/workflows/CI%20CPU%20testing/badge.svg" alt="CI CPU testing"></a>
<br>  
<a href="https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
 
</div>

</div>


## Introduction

This repository contains a two-stage-tracker. The detections generated by [YOLOv5](https://github.com/ultralytics/yolov5), a family of object detection architectures and models pretrained on the COCO dataset, are passed to a [Deep Sort algorithm](https://github.com/ZQPei/deep_sort_pytorch) which tracks the objects. It can track any object that your Yolov5 model was trained to detect.


## Tutorials

* [Yolov5 training on Custom Data (link to external repository)](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)&nbsp;
* [Deep Sort deep descriptor training (link to external repository)](https://github.com/ZQPei/deep_sort_pytorch#training-the-re-id-model)&nbsp;
* [Yolov5 deep_sort pytorch evaluation](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/wiki/Evaluation)&nbsp;



## Before you run the tracker

1. Clone the repository recursively:

`git clone --recurse-submodules https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch.git`

If you already cloned and forgot to use `--recurse-submodules` you can run `git submodule update --init`

2. Make sure that you fulfill all the requirements: Python 3.8 or later with all [requirements.txt](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch/blob/master/requirements.txt) dependencies installed, including torch>=1.7. To install, run:

`pip install -r requirements.txt`


## Tracking sources

Tracking can be run on most video formats

```bash
$ python track.py --source 0  # webcam
                           img.jpg  # image
                           vid.mp4  # video
                           path/  # directory
                           path/*.jpg  # glob
                           'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                           'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream
```


## Select a Yolov5 family model

There is a clear trade-off between model inference speed and accuracy. In order to make it possible to fulfill your inference speed/accuracy needs
you can select a Yolov5 family model for automatic download

```bash


$ python track.py --source 0 --yolo_weights yolov5n.pt --img 640
                                            yolov5s.pt
                                            yolov5m.pt
                                            yolov5l.pt 
                                            yolov5x.pt --img 1280
```

## Filter tracked classes

By default the tracker tracks all MS COCO classes.

If you only want to track persons I recommend you to get [these weights](https://drive.google.com/file/d/1gglIwqxaH2iTvy6lZlXuAcMpd_U0GCUb/view?usp=sharing) for increased performance

```bash
python3 track.py --source 0 --yolo_weights yolov5/weights/crowdhuman_yolov5m.pt --classes 0  # tracks persons, only
```

If you want to track a subset of the MS COCO classes, add their corresponding index after the classes flag

```bash
python3 track.py --source 0 --yolo_weights yolov5s.pt --classes 16 17  # tracks cats and dogs, only
```

[Here](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) is a list of all the possible objects that a Yolov5 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero.


## MOT compliant results

Can be saved to `inference/output` by 

```bash
python3 track.py --source ... --save-txt
```


## Cite

If you find this project useful in your research, please consider cite:

```latex
@misc{yolov5deepsort2020,
    title={Real-time multi-object tracker using YOLOv5 and deep sort},
    author={Mikel BrostrÃ¶m},
    howpublished = {\url{https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch}},
    year={2020}
}
```


def detect(opt):  
    out, source, yolo_weights, deep_sort_weights, show_vid, save_vid, save_txt, imgsz, evaluate, half = \  
        opt.output, opt.source, opt.yolo_weights, opt.deep_sort_weights, opt.show_vid, opt.save_vid, \  
            opt.save_txt, opt.imgsz, opt.evaluate, opt.half  
    webcam = source == '0' or source.startswith(  
        'rtsp') or source.startswith('http') or source.endswith('.txt')  
  
    # initialize deepsort  
    cfg = get_config()  
    cfg.merge_from_file(opt.config_deepsort)  
    attempt_download(deep_sort_weights, repo='mikel-brostrom/Yolov5_DeepSort_Pytorch')  
    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,  
                        max_dist=cfg.DEEPSORT.MAX_DIST, min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,  
                        max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,  
                        max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,  
                        use_cuda=True)  
  
    # Initialize  
    device = select_device(opt.device)  
    half &= device.type != 'cpu'  # half precision only supported on CUDA  
  
    # The MOT16 evaluation runs multiple inference streams in parallel, each one writing to    # its own .txt file. Hence, in that case, the output folder is not restored    if not evaluate:  
        if os.path.exists(out):  
            pass  
            shutil.rmtree(out)  # delete output folder  
        os.makedirs(out)  # make new output folder  
  
    # Load model    device = select_device(device)  
    model = DetectMultiBackend(opt.yolo_weights, device=device, dnn=opt.dnn)  
    stride, names, pt, jit, onnx = model.stride, model.names, model.pt, model.jit, model.onnx  
    imgsz = check_img_size(imgsz, s=stride)  # check image size  
  
    # Half    half &= pt and device.type != 'cpu'  # half precision only supported by PyTorch on CUDA  
    if pt:  
        model.model.half() if half else model.model.float()  
  
    # Set Dataloader  
    vid_path, vid_writer = None, None  
    # Check if environment supports image displays  
    if show_vid:  
        show_vid = check_imshow()  
  
    # Dataloader  
    if webcam:  
        view_img = check_imshow()  
        cudnn.benchmark = True  # set True to speed up constant image size inference  
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt and not jit)  
        bs = len(dataset)  # batch_size  
    else:  
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt and not jit)  
        bs = 1  # batch_size  
    vid_path, vid_writer = [None] * bs, [None] * bs  
  
    # Get names and colors  
    names = model.module.names if hasattr(model, 'module') else model.names  
  
    save_path = str(Path(out))  
    # extract what is in between the last '/' and last '.'  
    txt_file_name = source.split('/')[-1].split('.')[0]  
    txt_path = str(Path(out)) + '/' + txt_file_name + '.txt'  
  
    if pt and device.type != 'cpu':  
        model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.model.parameters())))  # warmup  
    dt, seen = [0.0, 0.0, 0.0], 0  
    for frame_idx, (path, img, im0s, vid_cap, s) in enumerate(dataset):  
        t1 = time_sync()  
        img = torch.from_numpy(img).to(device)  
        img = img.half() if half else img.float()  # uint8 to fp16/32  
        img /= 255.0  # 0 - 255 to 0.0 - 1.0  
        if img.ndimension() == 3:  
            img = img.unsqueeze(0)  
        t2 = time_sync()  
        dt[0] += t2 - t1  
  
        # Inference  
        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if opt.visualize else False  
        pred = model(img, augment=opt.augment, visualize=visualize)  
        t3 = time_sync()  
        dt[1] += t3 - t2  
  
        # Apply NMS  
        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, opt.classes, opt.agnostic_nms, max_det=opt.max_det)  
        dt[2] += time_sync() - t3  
  
        # Process detections  
        for i, det in enumerate(pred):  # detections per image  
            seen += 1  
            if webcam:  # batch_size >= 1  
                p, im0, frame = path[i], im0s[i].copy(), dataset.count  
                s += f'{i}: '  
            else:  
                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)  
  
            s += '%gx%g ' % img.shape[2:]  # print string  
            save_path = str(Path(out) / Path(p).name)  
  
            annotator = Annotator(im0, line_width=2, pil=not ascii)  
  
            if det is not None and len(det):  
                # Rescale boxes from img_size to im0 size  
                det[:, :4] = scale_boxes(  
                    img.shape[2:], det[:, :4], im0.shape).round()  
  
                # Print results  
                for c in det[:, -1].unique():  
                    n = (det[:, -1] == c).sum()  # detections per class  
                    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string  
  
                xywhs = xyxy2xywh(det[:, 0:4])  
                confs = det[:, 4]  
                clss = det[:, 5]  
  
                # pass detections to deepsort  
                outputs = deepsort.update(xywhs.cpu(), confs.cpu(), clss.cpu(), im0)  
                  
                # draw boxes for visualization  
                if len(outputs) > 0:  
                    for j, (output, conf) in enumerate(zip(outputs, confs)):   
                          
                        bboxes = output[0:4]  
                        id = output[4]  
                        cls = output[5]  
  
                        c = int(cls)  # integer class  
                        label = f'{id} {names[c]} {conf:.2f}'  
                        annotator.box_label(bboxes, label, color=colors(c, True))  
  
                        if save_txt:  
                            # to MOT format  
                            bbox_left = output[0]  
                            bbox_top = output[1]  
                            bbox_w = output[2] - output[0]  
                            bbox_h = output[3] - output[1]  
                            # Write MOT compliant results to file  
                            with open(txt_path, 'a') as f:  
                               f.write(('%g ' * 10 + '\n') % (frame_idx + 1, id, bbox_left,  
                                                           bbox_top, bbox_w, bbox_h, -1, -1, -1, -1))  # label format  
  
            else:  
                deepsort.increment_ages()  
  
            # Print time (inference-only)  
            LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')  
  
            # Stream results  
            im0 = annotator.result()  
            if show_vid:  
                cv2.imshow(p, im0)  
                if cv2.waitKey(1) == ord('q'):  # q to quit  
                    raise StopIteration  
  
            # Save results (image with detections)  
            if save_vid:  
                if vid_path != save_path:  # new video  
                    vid_path = save_path  
                    if isinstance(vid_writer, cv2.VideoWriter):  
                        vid_writer.release()  # release previous video writer  
                    if vid_cap:  # video  
                        fps = vid_cap.get(cv2.CAP_PROP_FPS)  
                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))  
                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  
                    else:  # stream  
                        fps, w, h = 30, im0.shape[1], im0.shape[0]  
                        save_path += '.mp4'  
  
                    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))  
                vid_writer.write(im0)  
  
    # Print results  
    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image  
    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)  
    if save_txt or save_vid:  
        print('Results saved to %s' % os.getcwd() + os.sep + out)  
        if platform == 'darwin':  # MacOS  
            os.system('open ' + save_path)  
  
  
if __name__ == '__main__':  
    parser = argparse.ArgumentParser()  
    parser.add_argument('--yolo_weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')  
    parser.add_argument('--deep_sort_weights', type=str, default='deep_sort_pytorch/deep_sort/deep/checkpoint/ckpt.t7', help='ckpt.t7 path')  
    # file/folder, 0 for webcam  
    parser.add_argument('--source', type=str, default='0', help='source')  
    parser.add_argument('--output', type=str, default='inference/output', help='output folder')  # output folder  
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')  
    parser.add_argument('--conf-thres', type=float, default=0.4, help='object confidence threshold')  
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')  
    parser.add_argument('--fourcc', type=str, default='mp4v', help='output video codec (verify ffmpeg support)')  
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')  
    parser.add_argument('--show-vid', action='store_true', help='display tracking video results')  
    parser.add_argument('--save-vid', action='store_true', help='save video tracking results')  
    parser.add_argument('--save-txt', action='store_true', help='save MOT compliant results to *.txt')  
    # class 0 is person, 1 is bycicle, 2 is car... 79 is oven  
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 16 17')  
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')  
    parser.add_argument('--augment', action='store_true', help='augmented inference')  
    parser.add_argument('--evaluate', action='store_true', help='augmented inference')  
    parser.add_argument("--config_deepsort", type=str, default="deep_sort_pytorch/configs/deep_sort.yaml")  
    parser.add_argument("--half", action="store_true", help="use FP16 half-precision inference")  
    parser.add_argument('--visualize', action='store_true', help='visualize features')  
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detection per image')  
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')  
    opt = parser.parse_args()  
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand  
  
    with torch.no_grad():  
        detect(opt)